{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepByStep(object):\n",
    "    \n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.train_loader = None\n",
    "        self.valid_loader = None\n",
    "        self.writer = None\n",
    "\n",
    "        self.losses = []\n",
    "        self.valid_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        self.train_step_fn = self._make_train_step_fn()\n",
    "        self.valid_step_fn = self._make_valid_step_fn()\n",
    "\n",
    "    def to(self, device):\n",
    "        try:\n",
    "            self.device = device\n",
    "            self.model.to(self.device)\n",
    "        except RuntimeError:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            print(f\"{device} not available, sending to {self.device}\")\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, valid_loader=None):\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter(f'{folder}/{name}_{suffix}')\n",
    "\n",
    "    def _make_train_step_fn(self):\n",
    "\n",
    "        def perform_train_step(x, y):\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            y_hat = self.model(x)\n",
    "\n",
    "            loss = self.loss_fn(y_hat, y)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            return loss.item()\n",
    "        \n",
    "        return perform_train_step\n",
    "\n",
    "    def _make_valid_step_fn(self):\n",
    "\n",
    "        def perform_val_step(x, y):\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            y_hat = self.model(x)\n",
    "\n",
    "            loss = self.loss_fn(y_hat, y)\n",
    "\n",
    "            return loss.item()\n",
    "        \n",
    "        return perform_val_step\n",
    "    \n",
    "    def _get_minibatch_loss(self, validation=False):\n",
    "\n",
    "        if validation:\n",
    "            step_fn = self.valid_step_fn\n",
    "            data_loader = self.valid_loader\n",
    "        else:\n",
    "            step_fn = self.train_step_fn\n",
    "            data_loader = self.train_loader\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "\n",
    "        mini_batch_losses = []\n",
    "\n",
    "        for x_batch, y_batch in data_loader:\n",
    "\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_losses.append(step_fn(x_batch, y_batch))\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def train(self, epochs, seed=42):\n",
    "\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            loss = self._get_minibatch_loss(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_loss = self._get_minibatch_loss(validation=True)\n",
    "                self.valid_losses.append(valid_loss)\n",
    "\n",
    "            if self.writer:\n",
    "                scalars = {'training': loss}\n",
    "\n",
    "                if valid_loss is not None:\n",
    "                    scalars.update({'validation': valid_loss})\n",
    "\n",
    "                self.writer.add_scalars(\n",
    "                    main_tag='loss',\n",
    "                    tag_scalar_dict=scalars,\n",
    "                    global_step=epoch\n",
    "                )\n",
    "\n",
    "        if self.writer:\n",
    "            self.writer.flush()\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        checkpoint = {\n",
    "            'epoch': self.total_epochs,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': self.losses,\n",
    "            'val_loss': self.valid_losses\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.valid_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.model.eval()\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        self.model.train()\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b')\n",
    "        if self.valid_loader:\n",
    "            plt.plot(self.valid_losses, label='Validation Loss', c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def add_graph(self):\n",
    "        if self.train_loader and self.writer:\n",
    "            x_dummy, y_dummy = next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model, x_dummy.to(self.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "ERROR_SCALE = 0.1\n",
    "TRUE_B = 1\n",
    "TRUE_W = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.rand(N, 1)\n",
    "error = ERROR_SCALE * np.random.randn(N, 1)\n",
    "y = TRUE_B + TRUE_W * X + error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i data_preparation/v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_configuration/v4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_configuration/v4.py\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(1, 1))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i model_configuration/v4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StepByStep' object has no attribute 'valid_losse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m sbs\u001b[39m.\u001b[39mset_loaders(train_loader, valid_loader)\n\u001b[0;32m      3\u001b[0m sbs\u001b[39m.\u001b[39mset_tensorboard(\u001b[39m'\u001b[39m\u001b[39mclassy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m sbs\u001b[39m.\u001b[39;49mtrain(epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[14], line 115\u001b[0m, in \u001b[0;36mStepByStep.train\u001b[1;34m(self, epochs, seed)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m    114\u001b[0m     valid_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_minibatch_loss(validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalid_losse\u001b[39m.\u001b[39mappend(valid_loss)\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter:\n\u001b[0;32m    118\u001b[0m     scalars \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m: loss}\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StepByStep' object has no attribute 'valid_losse'"
     ]
    }
   ],
   "source": [
    "sbs = StepByStep(model, loss_fn, optimizer)\n",
    "sbs.set_loaders(train_loader, valid_loader)\n",
    "sbs.set_tensorboard('classy')\n",
    "sbs.train(epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
